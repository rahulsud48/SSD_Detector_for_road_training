{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">5. Experiment (Training)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/.conda/envs/opcv/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib notebook\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from albumentations import (\n",
    "    CLAHE,\n",
    "    Blur,\n",
    "    OneOf,\n",
    "    Compose,\n",
    "    RGBShift,\n",
    "    GaussNoise,\n",
    "    RandomGamma,\n",
    "    RandomContrast,\n",
    "    RandomBrightness,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.transforms import HueSaturationValue\n",
    "from albumentations.augmentations.transforms import Normalize\n",
    "\n",
    "from trainer import Trainer, hooks, configuration\n",
    "from detector import Detector\n",
    "from trainer.utils import patch_configs\n",
    "from trainer.utils import setup_system\n",
    "from detection_loss import DetectionLoss\n",
    "from trainer.encoder import (\n",
    "    DataEncoder,\n",
    "    decode_boxes,\n",
    "    encode_boxes,\n",
    "    generate_anchors,\n",
    "    generate_anchor_grid,\n",
    ")\n",
    "from trainer.metrics import APEstimator\n",
    "from trainer.datasets import ListDataset\n",
    "# from trainer.data_set_downloader import DataSetDownloader \n",
    "from trainer.matplotlib_visualizer import MatplotlibVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5.1. Experiment Class</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_config: configuration.SystemConfig = configuration.SystemConfig(),\n",
    "        dataset_config: configuration.DatasetConfig = configuration.DatasetConfig(),  # pylint: disable=redefined-outer-name\n",
    "        dataloader_config: configuration.DataloaderConfig = configuration.DataloaderConfig(),  # pylint: disable=redefined-outer-name\n",
    "        optimizer_config: configuration.OptimizerConfig = configuration.OptimizerConfig(),  # pylint: disable=redefined-outer-name\n",
    "    ):\n",
    "        self.system_config = system_config\n",
    "        setup_system(system_config)\n",
    "        self.dataset_train = ListDataset(\n",
    "        root_dir=dataset_config.root_dir,\n",
    "        data_dir = 'export',\n",
    "        list_file='annots_converted_train.txt',\n",
    "            classes=[\n",
    "                \"__background__\",\n",
    "                \"biker\",\n",
    "                \"car\",\n",
    "                \"pedestrian\",\n",
    "                \"trafficLight\",\n",
    "                \"trafficLight-Green\",\n",
    "                \"trafficLight-GreenLeft\",\n",
    "                \"trafficLight-Red\",\n",
    "                \"trafficLight-RedLeft\",\n",
    "                \"trafficLight-Yellow\",\n",
    "                \"trafficLight-YellowLeft\",\n",
    "                \"truck\"\n",
    "            ],\n",
    "            mode='train',\n",
    "            transform=Compose(dataset_config.train_transforms),\n",
    "            input_size=300\n",
    "        )\n",
    "\n",
    "        self.loader_train = DataLoader(\n",
    "            dataset=self.dataset_train,\n",
    "            batch_size=dataloader_config.batch_size,\n",
    "            shuffle=True,\n",
    "#             collate_fn=self.dataset_train.collate_fn,\n",
    "            num_workers=dataloader_config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        self.dataset_test = ListDataset(\n",
    "            root_dir=dataset_config.root_dir,\n",
    "            data_dir = 'export',\n",
    "            list_file='annots_converted_train.txt',\n",
    "            classes=[\n",
    "                \"__background__\",\n",
    "                \"biker\",\n",
    "                \"car\",\n",
    "                \"pedestrian\",\n",
    "                \"trafficLight\",\n",
    "                \"trafficLight-Green\",\n",
    "                \"trafficLight-GreenLeft\",\n",
    "                \"trafficLight-Red\",\n",
    "                \"trafficLight-RedLeft\",\n",
    "                \"trafficLight-Yellow\",\n",
    "                \"trafficLight-YellowLeft\",\n",
    "                \"truck\"\n",
    "            ],\n",
    "            mode='val',\n",
    "            transform=Compose([Normalize(), ToTensorV2()]),\n",
    "            input_size=300\n",
    "        )\n",
    "        self.loader_test = DataLoader(\n",
    "            dataset=self.dataset_test,\n",
    "            batch_size=dataloader_config.batch_size,\n",
    "            shuffle=False,\n",
    "#             collate_fn=self.dataset_test.collate_fn,\n",
    "            num_workers=dataloader_config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        self.model = Detector(len(self.dataset_train.classes))\n",
    "        self.loss_fn = DetectionLoss(len(self.dataset_train.classes))\n",
    "        self.metric_fn = APEstimator(classes=self.dataset_test.classes)\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=optimizer_config.learning_rate,\n",
    "            weight_decay=optimizer_config.weight_decay,\n",
    "            momentum=optimizer_config.momentum\n",
    "        )\n",
    "        self.lr_scheduler = MultiStepLR(\n",
    "            self.optimizer, milestones=optimizer_config.lr_step_milestones, gamma=optimizer_config.lr_gamma\n",
    "        )\n",
    "        self.visualizer = MatplotlibVisualizer()\n",
    "\n",
    "    def run(self, trainer_config: configuration.TrainerConfig):\n",
    "        setup_system(self.system_config)\n",
    "        device = torch.device(trainer_config.device)\n",
    "        self.model = self.model.to(device)\n",
    "        self.loss_fn = self.loss_fn.to(device)\n",
    "\n",
    "        model_trainer = Trainer(\n",
    "            model=self.model,\n",
    "            loader_train=self.loader_train,\n",
    "            loader_test=self.loader_test,\n",
    "            loss_fn=self.loss_fn,\n",
    "            metric_fn=self.metric_fn,\n",
    "            optimizer=self.optimizer,\n",
    "            lr_scheduler=self.lr_scheduler,\n",
    "            device=device,\n",
    "            data_getter=itemgetter(\"image\"),\n",
    "            target_getter=itemgetter(\"target\"),\n",
    "            stage_progress=trainer_config.progress_bar,\n",
    "            get_key_metric=itemgetter(\"mAP\"),\n",
    "            visualizer=self.visualizer,\n",
    "            model_save_best=trainer_config.model_save_best,\n",
    "            model_saving_frequency=trainer_config.model_saving_frequency,\n",
    "            save_dir=trainer_config.model_dir\n",
    "        )\n",
    "\n",
    "        model_trainer.register_hook(\"train\", hooks.train_hook_detection)\n",
    "        model_trainer.register_hook(\"test\", hooks.test_hook_detection)\n",
    "        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_detection)\n",
    "        self.metrics = model_trainer.fit(trainer_config.epoch_num)\n",
    "        return self.metrics\n",
    "\n",
    "    def draw_bboxes(self, rows, columns, trainer_config: configuration.TrainerConfig):\n",
    "        # load the best model\n",
    "        if trainer_config.model_save_best:\n",
    "            self.model.load_state_dict(\n",
    "                torch.\n",
    "                load(os.path.join(trainer_config.model_dir, self.model.__class__.__name__) + '_best.pth')\n",
    "            )\n",
    "        # or use the last saved\n",
    "        self.model = self.model.eval()\n",
    "\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "\n",
    "        std = torch.Tensor(std)\n",
    "        mean = torch.Tensor(mean)\n",
    "\n",
    "        fig, ax = plt.subplots(\n",
    "            nrows=rows, ncols=columns, figsize=(10, 10), gridspec_kw={\n",
    "                'wspace': 0,\n",
    "                'hspace': 0.05\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for axi in ax.flat:\n",
    "            index = random.randrange(len(self.loader_test.dataset))\n",
    "\n",
    "            image, gt_boxes, _ = self.loader_test.dataset[index]\n",
    "\n",
    "            device = torch.device(trainer_config.device)\n",
    "            image = image.to(device).clone()\n",
    "\n",
    "            loc_preds, cls_preds = self.model(image.unsqueeze(0))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = image.cpu()\n",
    "                img.mul_(std[:, None, None]).add_(mean[:, None, None])\n",
    "                img = torch.clamp(img, min=0.0, max=1.0)\n",
    "                img = img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "                img = (img * 255.).astype(np.uint8)\n",
    "                gt_img = img.copy()\n",
    "                pred_img = img.copy()\n",
    "\n",
    "                for box in gt_boxes:\n",
    "                    gt_img = cv2.rectangle(\n",
    "                        gt_img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0),\n",
    "                        thickness=2\n",
    "                    )\n",
    "\n",
    "                encoder = DataEncoder((img.shape[1], img.shape[0]))\n",
    "                samples = encoder.decode(loc_preds, cls_preds)\n",
    "                c_dets = samples[0][1]  # detections for class == 1\n",
    "\n",
    "                if c_dets.size > 0:\n",
    "                    boxes = c_dets[:, :4]\n",
    "                    for box in boxes:\n",
    "                        pred_img = cv2.rectangle(\n",
    "                            pred_img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 0, 255),\n",
    "                            thickness=2\n",
    "                        )\n",
    "\n",
    "                merged_img = np.concatenate((gt_img, pred_img), axis=1)\n",
    "                axi.imshow(merged_img)\n",
    "                axi.axis('off')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5.3. Run Experiment</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataloader_config, trainer_config = patch_configs(epoch_num_to_set=10, batch_size_to_set=30)\n",
    "    # Downloading dataset\n",
    "#     DataSetDownloader(root_dir='data', dataset_title='PennFudanPed', download=True)\n",
    "    dataset_config = configuration.DatasetConfig(\n",
    "        root_dir=\"../../../Datasets/Road_Scene_Object_Detection\",\n",
    "        train_transforms=[\n",
    "            RandomBrightness(p=0.5),\n",
    "            RandomContrast(p=0.5),\n",
    "            OneOf([\n",
    "                RandomGamma(),\n",
    "                HueSaturationValue(hue_shift_limit=20, sat_shift_limit=50, val_shift_limit=50),\n",
    "                RGBShift()\n",
    "            ],\n",
    "                p=1),\n",
    "            OneOf([Blur(always_apply=True), GaussNoise(always_apply=True)], p=1),\n",
    "            CLAHE(),\n",
    "            Normalize(),\n",
    "            ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    optimizer_config = configuration.OptimizerConfig(\n",
    "        learning_rate=5e-3, \n",
    "        lr_step_milestones=[50], \n",
    "        lr_gamma=0.1, \n",
    "        momentum=0.9, \n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "    \n",
    "    experiment = Experiment(\n",
    "        dataset_config=dataset_config, \n",
    "        dataloader_config=dataloader_config, \n",
    "        optimizer_config=optimizer_config\n",
    "    )\n",
    "    \n",
    "#     # Run the experiment / start training\n",
    "#     experiment.run(trainer_config)\n",
    "    \n",
    "#     # how good our detector works by visualizing the results on the randomly chosen test images:\n",
    "#     experiment.draw_bboxes(4, 1, trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rahul/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/rahul/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/rahul/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/media/rahul/a079ceb2-fd12-43c5-b844-a832f31d5a39/Projects/autonomous_cars/Object_Detector_for_road/SSD_Detector_for_road_training/SSD_Trainer/trainer/datasets.py\", line 82, in __getitem__\n    img, boxes = resize(img, boxes, (size, size))\n  File \"/media/rahul/a079ceb2-fd12-43c5-b844-a832f31d5a39/Projects/autonomous_cars/Object_Detector_for_road/SSD_Detector_for_road_training/SSD_Trainer/trainer/utils.py\", line 91, in resize\n    boxes * torch.Tensor([scale_w, scale_h, scale_w, scale_h])\nRuntimeError: The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2419eb4585c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mone_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/opcv/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rahul/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/rahul/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/rahul/.conda/envs/opcv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/media/rahul/a079ceb2-fd12-43c5-b844-a832f31d5a39/Projects/autonomous_cars/Object_Detector_for_road/SSD_Detector_for_road_training/SSD_Trainer/trainer/datasets.py\", line 82, in __getitem__\n    img, boxes = resize(img, boxes, (size, size))\n  File \"/media/rahul/a079ceb2-fd12-43c5-b844-a832f31d5a39/Projects/autonomous_cars/Object_Detector_for_road/SSD_Detector_for_road_training/SSD_Trainer/trainer/utils.py\", line 91, in resize\n    boxes * torch.Tensor([scale_w, scale_h, scale_w, scale_h])\nRuntimeError: The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(experiment.loader_train)\n",
    "one_batch = next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, sometimes, predicted bounding boxes are not as tight or, on the opposite, not as wide as we wanted them to be.\n",
    "\n",
    "They can also be a little bit shifted from the ground-truth position or there can be one bounding box in the middle instead of two separated boxes for two people.\n",
    "\n",
    "All of the mentioned artifacts could be the result of:\n",
    "- not enough epochs for training, so we are facing underfitting;\n",
    "- not accurate choice of hyperparameters, so we are facing overfitting;\n",
    "- not precise annotation, so that the network couldn't learn the exact location of the bounding boxes."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
