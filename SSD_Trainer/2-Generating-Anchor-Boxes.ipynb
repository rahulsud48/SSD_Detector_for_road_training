{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">2. Generating Anchor Boxes</font>\n",
    "\n",
    "Our detector has 9 anchors for every feature map by default.\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w8-anchors.png' align='middle'>\n",
    "\n",
    "**What is feature map here?**\n",
    "\n",
    "Let's say `a` is an input image of dimension `256x256`, and it has two feature maps `b` (`8 x 8 feature map (grid)`) and `c` (`4 x 4 feature map (grid)`). One element of the feature map represents segments of pixels in the original image `a`.\n",
    "\n",
    "\n",
    "\n",
    "**Why 9?**\n",
    "\n",
    "To answer this question, let's take a look into DataEncoder class.\n",
    "We have 3 aspect ratios of sizes $1/2$, $1$ and $2$. For each size, there are `3` scales: $1, 2^{1/3}$ and $2^{2/3}$.\n",
    "These anchors of the appropriate sizes are generated for each of five feature maps we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Code\n",
    "import inspect\n",
    "\n",
    "from trainer.encoder import (\n",
    "    DataEncoder,\n",
    "    decode_boxes,\n",
    "    encode_boxes,\n",
    "    generate_anchors,\n",
    "    generate_anchor_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEncoder:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        self.anchor_areas = [8 * 8, 16 * 16., 32 * 32., 64 * 64., 128 * 128]  # p3 -> p7\n",
    "        self.aspect_ratios = [0.5, 1, 2]\n",
    "        self.scales = [1, pow(2, 1 / 3.), pow(2, 2 / 3.)]\n",
    "        num_fms = len(self.anchor_areas)\n",
    "        fm_sizes = [math.ceil(self.input_size[0] / pow(2., i + 3)) for i in range(num_fms)]\n",
    "        print(fm_sizes)\n",
    "        self.anchor_boxes = []\n",
    "        for i, fm_size in enumerate(fm_sizes):\n",
    "            anchors = generate_anchors(self.anchor_areas[i], self.aspect_ratios, self.scales)\n",
    "            anchor_grid = generate_anchor_grid(input_size, fm_size, anchors)\n",
    "            self.anchor_boxes.append(anchor_grid)\n",
    "        self.anchor_boxes = torch.cat(self.anchor_boxes, 0)\n",
    "        self.classes = [\"__background__\", \"person\"]\n",
    "\n",
    "    def encode(self, boxes, classes):\n",
    "        iou = compute_iou(boxes, self.anchor_boxes)\n",
    "        iou, ids = iou.max(1)\n",
    "        loc_targets = encode_boxes(boxes[ids], self.anchor_boxes)\n",
    "        cls_targets = classes[ids]\n",
    "        cls_targets[iou < 0.5] = -1\n",
    "        cls_targets[iou < 0.4] = 0\n",
    "\n",
    "        return loc_targets, cls_targets\n",
    "\n",
    "    def decode(self, loc_pred, cls_pred, cls_threshold=0.7, nms_threshold=0.3):\n",
    "        all_boxes = [[] for _ in range(len(loc_pred))]  # batch_size\n",
    "\n",
    "        for sample_id, (boxes, scores) in enumerate(zip(loc_pred, cls_pred)):\n",
    "            boxes = decode_boxes(boxes, self.anchor_boxes)\n",
    "\n",
    "            conf = scores.softmax(dim=1)\n",
    "            sample_boxes = [[] for _ in range(len(self.classes))]\n",
    "            for class_idx, class_name in enumerate(self.classes):\n",
    "                if class_name == '__background__':\n",
    "                    continue\n",
    "                class_conf = conf[:, class_idx]\n",
    "                ids = (class_conf > cls_threshold).nonzero().squeeze()\n",
    "                ids = [ids.tolist()]\n",
    "                keep = compute_nms(boxes[ids], class_conf[ids], threshold=nms_threshold)\n",
    "\n",
    "                conf_out, top_ids = torch.sort(class_conf[ids][keep], dim=0, descending=True)\n",
    "                boxes_out = boxes[ids][keep][top_ids]\n",
    "\n",
    "                boxes_out = boxes_out.cpu().numpy()\n",
    "                conf_out = conf_out.cpu().numpy()\n",
    "\n",
    "                c_dets = np.hstack((boxes_out, conf_out[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "                c_dets = c_dets[c_dets[:, 4].argsort()]\n",
    "                sample_boxes[class_idx] = c_dets\n",
    "\n",
    "            all_boxes[sample_id] = sample_boxes\n",
    "\n",
    "        return all_boxes\n",
    "\n",
    "    def get_num_anchors(self):\n",
    "        return len(self.aspect_ratios) * len(self.scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why have we chosen the following anchor area?**\n",
    "```\n",
    "anchor_areas = [8 * 8, 16 * 16., 32 * 32., 64 * 64., 128 * 128]  # p3 -> p7\n",
    "```\n",
    "The first anchor area is responsible for generating anchors for the first output layer of `FPN` and so on. \n",
    "\n",
    "```\n",
    "256/8 = 32\n",
    "\n",
    "256/16 = 16\n",
    "   .\n",
    "   .\n",
    "256/128 = 2\n",
    "```\n",
    "\n",
    "**So, how do we generate them?**\n",
    "\n",
    "We first generate our 9 anchors, knowing which areas it should cover, using predefined ratios and scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(anchor_area, aspect_ratios, scales):\n",
    "    anchors = []\n",
    "    for scale in scales:\n",
    "        for ratio in aspect_ratios:\n",
    "            h = math.sqrt(anchor_area/ratio)\n",
    "            w = math.sqrt(anchor_area*ratio)\n",
    "            x1 = (math.sqrt(anchor_area) - scale * w) * 0.5\n",
    "            y1 = (math.sqrt(anchor_area) - scale * h) * 0.5\n",
    "            x2 = (math.sqrt(anchor_area) + scale * w) * 0.5\n",
    "            y2 = (math.sqrt(anchor_area) + scale * h) * 0.5\n",
    "            anchors.append([x1, y1, x2, y2])\n",
    "    return torch.Tensor(anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature map we create a grid, that will allow us to densely put all of the possible boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_grid(input_size, fm_size, anchors):\n",
    "    grid_size = input_size[0] / fm_size\n",
    "    x, y = torch.meshgrid(torch.arange(0, fm_size) * grid_size, torch.arange(0, fm_size) * grid_size)\n",
    "    anchors = anchors.view(-1, 1, 1, 4)\n",
    "    xyxy = torch.stack([x, y, x, y], 2).float()\n",
    "    boxes = (xyxy + anchors).permute(2, 1, 0, 3).contiguous().view(-1, 4)\n",
    "    boxes[:, 0::2] = boxes[:, 0::2].clamp(0, input_size[0])\n",
    "    boxes[:, 1::2] = boxes[:, 1::2].clamp(0, input_size[1])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check the size of the anchor boxes. for input image size `3x256x256` and `3x300x300`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 16, 8, 4, 2]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-baf5fbd53f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mheight_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'anchor_boxes size: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f087ca85a3f9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfm_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_areas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspect_ratios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0manchor_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_anchor_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5df093a6b0e9>\u001b[0m in \u001b[0;36mgenerate_anchors\u001b[0;34m(anchor_area, aspect_ratios, scales)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_area\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0manchors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "height_width = (256, 256)\n",
    "\n",
    "data_encoder = DataEncoder(height_width)\n",
    "\n",
    "print('anchor_boxes size: {}'.format(data_encoder.anchor_boxes.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_boxes size: torch.Size([17451, 4])\n"
     ]
    }
   ],
   "source": [
    "height_width = (300, 300)\n",
    "\n",
    "data_encoder = DataEncoder(height_width)\n",
    "\n",
    "print('anchor_boxes size: {}'.format(data_encoder.anchor_boxes.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's compare anchor boxes size to detector network output size:**\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Image input size</h3></td> <td><h3>Anchor boxes size</h3></td> <td><h3>Detector Network output size</h3></td> </tr>\n",
    "        <tr><td><h3>(256, 256)</h3></td> <td><h3>[12276, 4]</h3></td> <td><h3>[batch_size, 12276, 4]</h3></td> </tr>\n",
    "        <tr><td><h3>(300, 300)</h3></td> <td><h3>[17451, 4]</h3></td> <td><h3>[batch_size, 17451, 4]</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n",
    "\n",
    "Basically, we want to encode our location target, such that the size location target becomes equal to the size of anchor boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
