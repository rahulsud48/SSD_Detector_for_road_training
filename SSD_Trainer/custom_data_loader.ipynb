{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self, root_dir, data_dir, list_file, classes, mode, transform, input_size):\n",
    "        '''\n",
    "        Args:\n",
    "          root_dir: (str) ditectory to images.\n",
    "          list_file: (str) path to index file.\n",
    "          train: (boolean) train or test.\n",
    "          transform: ([transforms]) image transforms.\n",
    "          input_size: (int) model input size.\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = classes\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.fnames = []\n",
    "        self.boxes = []\n",
    "        self.labels = []\n",
    "\n",
    "        list_file_path = os.path.join(root_dir, list_file)\n",
    "        with open(list_file_path) as f:\n",
    "            lines = f.readlines()\n",
    "            self.num_samples = len(lines)\n",
    "\n",
    "        for line in lines:\n",
    "            splited = line.strip().split()\n",
    "            self.fnames.append(splited[0])\n",
    "            num_boxes = (len(splited) - 1) // 5\n",
    "            box = []\n",
    "            label = []\n",
    "            for i in range(num_boxes):\n",
    "                xmin = splited[1 + 5 * i]\n",
    "                ymin = splited[2 + 5 * i]\n",
    "                xmax = splited[3 + 5 * i]\n",
    "                ymax = splited[4 + 5 * i]\n",
    "                class_label = splited[5 + 5 * i]\n",
    "                box.append([float(xmin), float(ymin), float(xmax), float(ymax)])\n",
    "                label.append(int(class_label))\n",
    "            self.boxes.append(torch.Tensor(box))\n",
    "            self.labels.append(torch.LongTensor(label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Load image.\n",
    "\n",
    "        Args:\n",
    "          idx: (int) image index.\n",
    "\n",
    "        Returns:\n",
    "          img: (tensor) image tensor.\n",
    "          loc_targets: (tensor) location targets.\n",
    "          cls_targets: (tensor) class label targets.\n",
    "        '''\n",
    "        # Load image and boxes.\n",
    "        path = os.path.join(self.root_dir, self.data_dir, self.fnames[idx])\n",
    "        img = cv2.imread(path)\n",
    "        if img is None or np.prod(img.shape) == 0:\n",
    "            print('cannot load image from path: ', path)\n",
    "            sys.exit(-1)\n",
    "\n",
    "        img = img[..., ::-1]\n",
    "\n",
    "        boxes = self.boxes[idx].clone()\n",
    "        labels = self.labels[idx]\n",
    "        size = self.input_size\n",
    "\n",
    "        # Resize & Flip\n",
    "        img, boxes = resize(img, boxes, (size, size))\n",
    "        if self.mode == 'train':\n",
    "            img, boxes = random_flip(img, boxes)\n",
    "        # Data augmentation.\n",
    "        img = np.array(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "\n",
    "        return img, boxes, labels\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''Pad images and encode targets.\n",
    "\n",
    "        As for images are of different sizes, we need to pad them to the same size.\n",
    "\n",
    "        Args:\n",
    "          batch: (list) of images, cls_targets, loc_targets.\n",
    "\n",
    "        Returns:\n",
    "          padded images, stacked cls_targets, stacked loc_targets.\n",
    "        '''\n",
    "        imgs = [x[0] for x in batch]\n",
    "        boxes = [x[1] for x in batch]\n",
    "        labels = [x[2] for x in batch]\n",
    "\n",
    "        h = w = self.input_size\n",
    "        num_imgs = len(imgs)\n",
    "        inputs = torch.zeros(num_imgs, 3, w, h)\n",
    "        encoder = DataEncoder((w, h))\n",
    "        loc_targets = []\n",
    "        cls_targets = []\n",
    "        for i in range(num_imgs):\n",
    "            inputs[i] = imgs[i]\n",
    "            loc_target, cls_target = encoder.encode(boxes[i], labels[i])\n",
    "            loc_targets.append(loc_target)\n",
    "            cls_targets.append(cls_target)\n",
    "        return inputs, torch.stack(loc_targets), torch.stack(cls_targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainerConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-48ca706cce12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mpatch_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num_to_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrainerConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_to_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataloaderConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\" Patches configs if cuda is not available\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mreturns\u001b[0m \u001b[0mpatched\u001b[0m \u001b[0mdataloader_config\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrainer_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainerConfig' is not defined"
     ]
    }
   ],
   "source": [
    "def patch_configs(epoch_num_to_set=TrainerConfig.epoch_num, batch_size_to_set=DataloaderConfig.batch_size):\n",
    "    \"\"\" Patches configs if cuda is not available\n",
    "\n",
    "    Returns:\n",
    "        returns patched dataloader_config and trainer_config\n",
    "\n",
    "    \"\"\"\n",
    "    # default experiment params\n",
    "    num_workers_to_set = DataloaderConfig.num_workers\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 1\n",
    "\n",
    "    dataloader_config = DataloaderConfig(batch_size=batch_size_to_set, num_workers=num_workers_to_set)\n",
    "    trainer_config = TrainerConfig(device=device, epoch_num=epoch_num_to_set, progress_bar=True)\n",
    "    return dataloader_config, trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patch_configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5c329ee2d870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataloader_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num_to_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_to_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m dataset_config = configuration.DatasetConfig(\n\u001b[1;32m      3\u001b[0m     \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../../../Datasets/Road_Scene_Object_Detection\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     train_transforms=[\n\u001b[1;32m      5\u001b[0m         \u001b[0mRandomBrightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'patch_configs' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader_config, trainer_config = patch_configs(epoch_num_to_set=10, batch_size_to_set=1)\n",
    "dataset_config = configuration.DatasetConfig(\n",
    "    root_dir=\"../../../Datasets/Road_Scene_Object_Detection\",\n",
    "    train_transforms=[\n",
    "        RandomBrightness(p=0.5),\n",
    "        RandomContrast(p=0.5),\n",
    "        OneOf([\n",
    "            RandomGamma(),\n",
    "            HueSaturationValue(hue_shift_limit=20, sat_shift_limit=50, val_shift_limit=50),\n",
    "            RGBShift()\n",
    "        ],\n",
    "            p=1),\n",
    "        OneOf([Blur(always_apply=True), GaussNoise(always_apply=True)], p=1),\n",
    "        CLAHE(),\n",
    "        Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-37a61471d182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_train = ListDataset(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'export'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlist_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'annots_converted_train.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_config' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_train = ListDataset(\n",
    "    root_dir=dataset_config.root_dir,\n",
    "    data_dir = 'export',\n",
    "    list_file='annots_converted_train.txt',\n",
    "\n",
    "    classes=[\n",
    "        \"__background__\",\n",
    "        \"biker\",\n",
    "        \"car\",\n",
    "        \"pedestrian\",\n",
    "        \"trafficLight\",\n",
    "        \"trafficLight-Green\",\n",
    "        \"trafficLight-GreenLeft\",\n",
    "        \"trafficLight-Red\",\n",
    "        \"trafficLight-RedLeft\",\n",
    "        \"trafficLight-Yellow\",\n",
    "        \"trafficLight-YellowLeft\",\n",
    "        \"truck\"\n",
    "    ],\n",
    "    mode='train',\n",
    "    transform=Compose(dataset_config.train_transforms),\n",
    "    input_size=300\n",
    ")\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=dataloader_config.batch_size,\n",
    "    shuffle=True,\n",
    "#     collate_fn=dataset_train.collate_fn,\n",
    "    num_workers=dataloader_config.num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
